{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "xy= pd.read_csv('ADNI_adnimerge_20170629_QT-freeze.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY=xy[['RID', 'VISCODE','AGE',       'PTGENDER', 'PTEDUCAT',         'APOE4'\n",
    "       , 'FDG', 'PIB', 'AV45', 'ABETA', 'PTAU', 'TAU', 'CDRSB',\n",
    "       'RAVLTimmediate', 'RAVLTlearning',\n",
    "       'RAVLTforgetting', 'RAVLTpercforgetting', 'FAQ', 'MOCA', 'EcogPtMem',\n",
    "       'EcogPtLang', 'EcogPtVisspat', 'EcogPtPlan', 'EcogPtOrgan',\n",
    "       'EcogPtDivatt', 'EcogPtTotal', 'EcogSPMem', 'EcogSPLang',\n",
    "       'EcogSPVisspat', 'EcogSPPlan', 'EcogSPOrgan', 'EcogSPDivatt',\n",
    "       'EcogSPTotal',  'Ventricles', 'Hippocampus',\n",
    "       'WholeBrain', 'Entorhinal', 'Fusiform', 'MidTemp', 'ICV','MMSE','DX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xybl=xy[xy['VISCODE']=='bl']\n",
    "dx=xybl['DX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12749, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping values which are not meaningful\n",
    "# ABETHA\n",
    "XY['ABETA']=XY['ABETA'].replace('>1700',None)\n",
    "XY['ABETA']=XY['ABETA'].replace('<200',None)\n",
    "\n",
    "# PTAU\n",
    "XY['PTAU']=XY['PTAU'].replace('>120',None)\n",
    "XY['PTAU']=XY['PTAU'].replace('<8',None)\n",
    "# TAU\n",
    "\n",
    "XY['TAU']=XY['TAU'].replace('>1300',None)\n",
    "XY['TAU']=XY['TAU'].replace('<80',None)\n",
    "\n",
    "XY.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = XY.drop(XY[XY['Ventricles']==0].index)\n",
    "XY = XY.dropna(subset=['DX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = XY.drop(XY[XY['DX']=='MCI to Dementia'].index)\n",
    "XY = XY.drop(XY[XY['DX']=='NL to MCI'].index)\n",
    "XY = XY.drop(XY[XY['DX']=='MCI to NL'].index)\n",
    "XY = XY.drop(XY[XY['DX']=='Dementia to MCI'].index)\n",
    "XY = XY.drop(XY[XY['DX']=='NL to Dementia'].index)\n",
    "XY = XY.drop(XY[XY['MMSE'].isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMSE nan\n",
    "# XY = XY.dropna(subset=['MMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining value types which we deal with them in the above section\n",
    "# ABETA\n",
    "XY['ABETA']=(XY['ABETA'].astype(float))\n",
    "XY = XY.infer_objects()\n",
    "# PTAU\n",
    "XY['PTAU']=(XY['PTAU'].astype(float))\n",
    "# TAU\n",
    "XY['TAU']=(XY['TAU'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY=XY.set_index('RID',drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3326f4f4ca11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YlGnBu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[1;32m    516\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                           yticklabels, mask)\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Determine good default values for the colormapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         self._determine_cmap_params(plot_data, vmin, vmax,\n\u001b[0;32m--> 167\u001b[0;31m                                     cmap, center, robust)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Sort out the annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_determine_cmap_params\u001b[0;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                cmap, center, robust):\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m\"\"\"Use some heuristics to set good defaults for colorbar and range.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mcalc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcalc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "XY = XY[XY['VISCODE']=='bl']\n",
    "# XY.columns\n",
    "XY = XY.drop(['RID','MMSE','DX'],axis=1)\n",
    "XY = XY.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c980bd259a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"YlGnBu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n\u001b[1;32m    516\u001b[0m                           \u001b[0mannot_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_kws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticklabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                           yticklabels, mask)\n\u001b[0m\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;31m# Add the pcolormesh kwargs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Determine good default values for the colormapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         self._determine_cmap_params(plot_data, vmin, vmax,\n\u001b[0;32m--> 167\u001b[0;31m                                     cmap, center, robust)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# Sort out the annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_determine_cmap_params\u001b[0;34m(self, plot_data, vmin, vmax, cmap, center, robust)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                cmap, center, robust):\n\u001b[1;32m    203\u001b[0m         \u001b[0;34m\"\"\"Use some heuristics to set good defaults for colorbar and range.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mcalc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrobust\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcalc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "ax = sns.heatmap(XY, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = preprocessing.LabelEncoder()\n",
    "# gender_encoded=le.fit_transform(XY['PTGENDER'])\n",
    "# XY=XY.drop(['PTGENDER'], axis=1)\n",
    "# enc = OneHotEncoder(sparse=False)\n",
    "# gender_encoded = gender_encoded.reshape(len(gender_encoded), 1)\n",
    "# onehot_encoded = enc.fit_transform(gender_encoded)\n",
    "# temp=pd.DataFrame(onehot_encoded)\n",
    "# temp=temp.reset_index(drop=True)\n",
    "# XY=XY.reset_index(drop=True)\n",
    "# XY=pd.concat([XY, temp], axis=1,  join_axes=[ XY.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=XY[XY['VISCODE']=='bl'][['MMSE','RID']]\n",
    "x1=XY[XY['VISCODE']=='bl']\n",
    "# dx1=XY[XY['VISCODE']=='bl']['DX']\n",
    "# x1=x1.drop(['VISCODE','MMSE'], axis=1)\n",
    "x1=x1.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX','PTGENDER']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX']).AGE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX']).AGE.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX']).PTEDUCAT.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX']).PTEDUCAT.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.groupby(['DX']).MMSE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=XY[XY['VISCODE']=='m06'][['MMSE','RID']]\n",
    "x2=XY[XY['VISCODE']=='m06']\n",
    "# dx2=XY[XY['VISCODE']=='m06']['DX']\n",
    "x2=x2.drop(['VISCODE','MMSE'], axis=1)\n",
    "x2=x2.fillna(0)\n",
    "x2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x2=x2.rename(index=str, columns ={'RID':'RID', 'FDG':'FDG_2', 'PIB':'PIB_2', 'AV45':'AV45_2', 'ABETA':'ABETA_2',\n",
    "        'PTAU':'PTAU_2', 'TAU': 'TAU_2', 'CDRSB':'CDRSB_2',\n",
    "       'RAVLTimmediate':'RAVLTimmediate_2', 'RAVLTlearning':'RAVLTlearning_2', 'RAVLTforgetting':'RAVLTforgetting_2',\n",
    "       'RAVLTpercforgetting':'RAVLTpercforgetting_2', 'FAQ':'FAQ_2', 'MOCA':'MOCA_2',\n",
    "       'EcogPtMem':'EcogPtMem_2', 'EcogPtLang':'EcogPtLang_2',\n",
    "       'EcogPtVisspat':'EcogPtVisspat_2', 'EcogPtPlan': 'EcogPtPlan_2', 'EcogPtOrgan':'EcogPtOrgan_2', 'EcogPtDivatt':'EcogPtDivatt_2',\n",
    "       'EcogPtTotal':'EcogPtTotal_2', 'EcogSPMem':'EcogSPMem_2', 'EcogSPLang':'EcogSPLang_2', 'EcogSPVisspat':'EcogSPVisspat_2', 'EcogSPPlan':'EcogSPPlan_2',\n",
    "       'EcogSPOrgan':'EcogSPOrgan_2', 'EcogSPDivatt':'EcogSPDivatt_2', 'EcogSPTotal':'EcogSPTotal_2', 'Ventricles':'Ventricles_2',\n",
    "       'Hippocampus':'Hippocampus_2', 'WholeBrain':'WholeBrain_2', 'Entorhinal':'Entorhinal_2', 'Fusiform':'Fusiform_2', 'MidTemp':'MidTemp_2',\n",
    "       'ICV':'ICV_2','DX':'DX_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3=XY[XY['VISCODE']=='m12'][['MMSE','RID']]\n",
    "x3=XY[XY['VISCODE']=='m12']\n",
    "x3=x3.drop(['VISCODE','MMSE'], axis=1)\n",
    "x3=x3.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3=x3.rename(index=str, columns ={'RID':'RID', 'FDG':'FDG_3', 'PIB':'PIB_3', 'AV45':'AV45_3', 'ABETA':'ABETA_3',\n",
    "        'PTAU':'PTAU_3', 'TAU': 'TAU_3', 'CDRSB':'CDRSB_3',\n",
    "       'RAVLTimmediate':'RAVLTimmediate_3', 'RAVLTlearning':'RAVLTlearning_3', 'RAVLTforgetting':'RAVLTforgetting_3',\n",
    "       'RAVLTpercforgetting':'RAVLTpercforgetting_3', 'FAQ':'FAQ_3', 'MOCA':'MOCA_3',\n",
    "       'EcogPtMem':'EcogPtMem_3', 'EcogPtLang':'EcogPtLang_3',\n",
    "       'EcogPtVisspat':'EcogPtVisspat_3', 'EcogPtPlan': 'EcogPtPlan_3', 'EcogPtOrgan':'EcogPtOrgan_3', 'EcogPtDivatt':'EcogPtDivatt_3',\n",
    "       'EcogPtTotal':'EcogPtTotal_3', 'EcogSPMem':'EcogSPMem_3', 'EcogSPLang':'EcogSPLang_3', 'EcogSPVisspat':'EcogSPVisspat_3', 'EcogSPPlan':'EcogSPPlan_3',\n",
    "       'EcogSPOrgan':'EcogSPOrgan_3', 'EcogSPDivatt':'EcogSPDivatt_3', 'EcogSPTotal':'EcogSPTotal_3', 'Ventricles':'Ventricles_3',\n",
    "       'Hippocampus':'Hippocampus_3', 'WholeBrain':'WholeBrain_3', 'Entorhinal':'Entorhinal_3', 'Fusiform':'Fusiform_3', 'MidTemp':'MidTemp_3',\n",
    "       'ICV':'ICV_3','DX':'DX_3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4=XY[XY['VISCODE']=='m24'][['MMSE','RID']]\n",
    "x4=XY[XY['VISCODE']=='m24']\n",
    "x4=x4.drop(['VISCODE','MMSE'], axis=1)\n",
    "x4=x4.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4=x4.rename(index=str, columns ={'RID':'RID', 'FDG':'FDG_4', 'PIB':'PIB_4', 'AV45':'AV45_4', 'ABETA':'ABETA_4',\n",
    "        'PTAU':'PTAU_4', 'TAU': 'TAU_4', 'CDRSB':'CDRSB_4',\n",
    "       'RAVLTimmediate':'RAVLTimmediate_4', 'RAVLTlearning':'RAVLTlearning_4', 'RAVLTforgetting':'RAVLTforgetting_4',\n",
    "       'RAVLTpercforgetting':'RAVLTpercforgetting_4', 'FAQ':'FAQ_4', 'MOCA':'MOCA_4',\n",
    "       'EcogPtMem':'EcogPtMem_4', 'EcogPtLang':'EcogPtLang_4',\n",
    "       'EcogPtVisspat':'EcogPtVisspat_4', 'EcogPtPlan': 'EcogPtPlan_4', 'EcogPtOrgan':'EcogPtOrgan_4', 'EcogPtDivatt':'EcogPtDivatt_4',\n",
    "       'EcogPtTotal':'EcogPtTotal_4', 'EcogSPMem':'EcogSPMem_4', 'EcogSPLang':'EcogSPLang_4', 'EcogSPVisspat':'EcogSPVisspat_4', 'EcogSPPlan':'EcogSPPlan_4',\n",
    "       'EcogSPOrgan':'EcogSPOrgan_4', 'EcogSPDivatt':'EcogSPDivatt_4', 'EcogSPTotal':'EcogSPTotal_4', 'Ventricles':'Ventricles_4',\n",
    "       'Hippocampus':'Hippocampus_4', 'WholeBrain':'WholeBrain_4', 'Entorhinal':'Entorhinal_4', 'Fusiform':'Fusiform_4', 'MidTemp':'MidTemp_4',\n",
    "       'ICV':'ICV_4','DX':'DX_4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y5=XY[XY['VISCODE']=='m36'][['MMSE','RID']]\n",
    "x5=XY[XY['VISCODE']=='m36']\n",
    "x5=x5.drop(['VISCODE','MMSE'], axis=1)\n",
    "x5=x5.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x5=x5.rename(index=str, columns ={'RID':'RID', 'FDG':'FDG_5', 'PIB':'PIB_5', 'AV45':'AV45_5', 'ABETA':'ABETA_5',\n",
    "        'PTAU':'PTAU_5', 'TAU': 'TAU_5', 'CDRSB':'CDRSB_5',\n",
    "       'RAVLTimmediate':'RAVLTimmediate_5', 'RAVLTlearning':'RAVLTlearning_5', 'RAVLTforgetting':'RAVLTforgetting_5',\n",
    "       'RAVLTpercforgetting':'RAVLTpercforgetting_5', 'FAQ':'FAQ_5', 'MOCA':'MOCA_5',\n",
    "       'EcogPtMem':'EcogPtMem_5', 'EcogPtLang':'EcogPtLang_5',\n",
    "       'EcogPtVisspat':'EcogPtVisspat_5', 'EcogPtPlan': 'EcogPtPlan_5', 'EcogPtOrgan':'EcogPtOrgan_5', 'EcogPtDivatt':'EcogPtDivatt_5',\n",
    "       'EcogPtTotal':'EcogPtTotal_5', 'EcogSPMem':'EcogSPMem_5', 'EcogSPLang':'EcogSPLang_5', 'EcogSPVisspat':'EcogSPVisspat_5', 'EcogSPPlan':'EcogSPPlan_5',\n",
    "       'EcogSPOrgan':'EcogSPOrgan_5', 'EcogSPDivatt':'EcogSPDivatt_5', 'EcogSPTotal':'EcogSPTotal_5', 'Ventricles':'Ventricles_5',\n",
    "       'Hippocampus':'Hippocampus_5', 'WholeBrain':'WholeBrain_5', 'Entorhinal':'Entorhinal_5', 'Fusiform':'Fusiform_5', 'MidTemp':'MidTemp_5',\n",
    "       'ICV':'ICV_5','DX':'DX_5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLSTM=x1.merge(x2,how='inner', left_on='RID', right_on='RID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dfs = [x1,x2,x3]#,x4,x5\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,on='RID',how='inner'), dfs)\n",
    "# s=pd.merge(x1,x2,on='RID',how='inner')\n",
    "# df_final=pd.merge(s,x3,on='RID',how='inner')\n",
    "\n",
    "dfsy= [y3,y4,y5]#y1,y2\n",
    "df_final_y = reduce(lambda left,right: pd.merge(left,right,on='RID',how='inner'), dfsy)\n",
    "# s=pd.merge(y3,y4,on='RID',how='inner')\n",
    "# df_final_y=pd.merge(s,y5,on='RID',how='inner')\n",
    "\n",
    "DFXY= [x1,x2,x3,y3,y4,y5]\n",
    "df_xy = reduce(lambda left,right: pd.merge(left,right,on='RID',how='inner'), DFXY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xy stats\n",
    "\n",
    "print(XY['DX'].value_counts())\n",
    "XY.groupby('DX').MMSE.mean()\n",
    "XY.groupby('DX').MMSE.std()\n",
    "XY.groupby('age').MMSE.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xy.columns\n",
    "\n",
    "df_xy_X=df_xy.iloc[:,0:-3]\n",
    "df_xy_y=df_xy.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_xy_X, df_xy_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final['DX'].value_counts()\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# df_final['DX']=le.fit_transform(df_final['DX'])\n",
    "# # ventricle= df_final[['Ventricles']]#,'Ventricles_2','Ventricles_3','Ventricles_4','Ventricles_5']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final_y[df_final_y['MMSE'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# # scaler = StandardScaler(with_mean=False)\n",
    "\n",
    "\n",
    "# # df_X =df_final.copy()\n",
    "# # df_DX = df_X[['DX','DX_2','DX_3']]\n",
    "# # df_X=df_X.drop(['DX','DX_2','DX_3'], axis=1)\n",
    "\n",
    "# # df_X=scaler.fit_transform(df_X)\n",
    "# # df_final_y=scaler.fit_transform(df_final_y)\n",
    "\n",
    "# y =df_final['RAVLTimmediate']\n",
    "\n",
    "# #         'PTAU':'PTAU_5', 'TAU': 'TAU_5', 'CDRSB':'CDRSB_5',\n",
    "# #        'RAVLTimmediate':'RAVLTimmediate_5', 'RAVLTlearning':'RAVLTlearning_5', 'RAVLTforgetting':'RAVLTforgetting_5',\n",
    "# #        'RAVLTpercforgetting':'RAVLTpercforgetting_5', 'FAQ':'FAQ_5', 'MOCA':'MOCA_5',\n",
    "# #        'EcogPtMem':'EcogPtMem_5', 'EcogPtLang':'EcogPtLang_5',\n",
    "# #        'EcogPtVisspat':'EcogPtVisspat_5', 'EcogPtPlan': 'EcogPtPlan_5', 'EcogPtOrgan':'EcogPtOrgan_5', 'EcogPtDivatt':'EcogPtDivatt_5',\n",
    "# #        'EcogPtTotal':'EcogPtTotal_5', 'EcogSPMem':'EcogSPMem_5', 'EcogSPLang':'EcogSPLang_5', 'EcogSPVisspat':'EcogSPVisspat_5', 'EcogSPPlan':'EcogSPPlan_5',\n",
    "# #        'EcogSPOrgan':'EcogSPOrgan_5', 'EcogSPDivatt':'EcogSPDivatt_5', 'EcogSPTotal':'EcogSPTotal_5', 'Ventricles':'Ventricles_5',\n",
    "# #        'Hippocampus':'Hippocampus_5', 'WholeBrain':'WholeBrain_5', 'Entorhinal':'Entorhinal_5', 'Fusiform':'Fusiform_5', 'MidTemp':'MidTemp_5',\n",
    "# #        'ICV':'ICV_5','DX':'DX_5'})\n",
    "\n",
    "# # y=scaler.fit_transform(np.reshape(y,(-1,1)))\n",
    "# # y=y[110:300]\n",
    "# x = df_final['WholeBrain']#\n",
    "# # x=x[110:300]\n",
    "# colors = df_final['DX']\n",
    "# # colors = colors.iloc[110:300]\n",
    "# colors=colors+.15\n",
    "# a =  df_final['CDRSB']\n",
    "# area = a*20#.iloc[110:300]\n",
    "\n",
    "# plt.scatter(x, y,s=area, c=colors, alpha=0.5)# \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=df_xy_X\n",
    "df_final=df_final.drop(['DX','DX_2','DX_3'], axis=1)#,'DX_4','DX_5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train=X_train.drop(['DX','DX_2','DX_3'], axis=1)\n",
    "X_test=X_test.drop(['DX','DX_2','DX_3'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ventricle= df_final[['Ventricles','Ventricles_2','Ventricles_3','Ventricles_4','Ventricles_5']]\n",
    "# ventricle2=ventricle[ventricle.Ventricles !=0]\n",
    "# vetricle3=ventricle2[ventricle2.Ventricles.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = (scaler.fit_transform(X_train))\n",
    "X_train_3d= np.asarray(X_train[0:-3,1:]).reshape((-1,3,34))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test = (scaler.fit_transform(X_test))\n",
    "X_test_3d= np.asarray(X_test[:,1:]).reshape((-1,3,34))\n",
    "\n",
    "\n",
    "y_train = (scaler.fit_transform(y_train))\n",
    "y_train_3d= np.asarray(y_train[0:-3]).reshape((-1,3,1))\n",
    "\n",
    "# X_val_3d = X_train_3d[0:53,:,:]\n",
    "# X_train_3d= X_train_3d[53:,:,:]\n",
    "\n",
    "# y_val_3d = y_train_3d[0:53,:,:]\n",
    "# y_train_3d = y_train_3d[53:,:,:]\n",
    "\n",
    "\n",
    "y_test = (scaler.fit_transform(y_test))\n",
    "y_test_3d= np.asarray(y_test).reshape((-1,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_3d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one output for each input time step\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU,Bidirectional\n",
    "from numpy import array\n",
    "from keras.layers import TimeDistributed\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()#Bidirectional\n",
    "model.add(LSTM(5,activation='relu', return_sequences=True, input_shape=(3,34)))#,stateful=True , batch_input_shape = (10,3,34)\n",
    "# model.add(LSTM(30,activation='relu', return_sequences=True, input_shape=(3,34)))\n",
    "model.add(LSTM(3,activation='linear', return_sequences=True, input_shape=(3,34)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# fit network, validation_data=(X_train_3d, y_train_3d)\n",
    "history = model.fit(X_train_3d, y_train_3d, epochs=150, validation_split=0.3,\n",
    "                    batch_size=10, verbose=0, shuffle=True)\n",
    "# plot history\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "# X_test_3d = X_train_3d\n",
    "# y_test_3d = y_train_3d\n",
    "\n",
    "y_dim1,y_dim2,y_dim3 = y_test_3d.shape\n",
    "y_test_2d= np.asarray(y_test_3d).reshape((y_dim1, y_dim2))\n",
    "y_test_2i = scaler.inverse_transform(y_test_2d)\n",
    "yhat = model.predict(X_test_3d)\n",
    "yhat_2= np.asarray(yhat).reshape((y_dim1, y_dim2))\n",
    "yhat_2_i = scaler.inverse_transform(yhat_2)\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,0], yhat_2_i[:,0])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,1], yhat_2_i[:,1])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,2], yhat_2_i[:,2])))\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test_2i, yhat_2_i))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,0], yhat_2_i[:,0]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,1], yhat_2_i[:,1]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,2], yhat_2_i[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of one output for each input time step\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU,Bidirectional\n",
    "from numpy import array\n",
    "from keras.layers import TimeDistributed\n",
    "# define model where LSTM is also output layer\n",
    "model = Sequential()#Bidirectional\n",
    "model.add(GRU(5,activation='relu', return_sequences=True, input_shape=(3,34)))#,stateful=True , batch_input_shape = (10,3,34)\n",
    "# model.add(LSTM(30,activation='relu', return_sequences=True, input_shape=(3,34)))\n",
    "model.add(GRU(3,activation='linear', return_sequences=True, input_shape=(3,34)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(TimeDistributed(Dense(1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# fit network, validation_data=(X_train_3d, y_train_3d)\n",
    "history = model.fit(X_train_3d, y_train_3d, epochs=150, validation_split=0.3,\n",
    "                    batch_size=10, verbose=0, shuffle=True)\n",
    "# plot history\n",
    "import matplotlib.pyplot as pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "# X_test_3d = X_train_3d\n",
    "# y_test_3d = y_train_3d\n",
    "\n",
    "y_dim1,y_dim2,y_dim3 = y_test_3d.shape\n",
    "y_test_2d= np.asarray(y_test_3d).reshape((y_dim1, y_dim2))\n",
    "y_test_2i = scaler.inverse_transform(y_test_2d)\n",
    "yhat = model.predict(X_test_3d)\n",
    "yhat_2= np.asarray(yhat).reshape((y_dim1, y_dim2))\n",
    "yhat_2_i = scaler.inverse_transform(yhat_2)\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,0], yhat_2_i[:,0])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,1], yhat_2_i[:,1])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_2i[:,2], yhat_2_i[:,2])))\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test_2i, yhat_2_i))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,0], yhat_2_i[:,0]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,1], yhat_2_i[:,1]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_2i[:,2], yhat_2_i[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_2_i[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_2i[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test_2i,yhat_2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "# Create linear regression object\n",
    "# regr = linear_model.LinearRegression()\n",
    "regr =Ridge(alpha=1.0)\n",
    "regr.fit(X_train, y_train[:,0])\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred1= np.asarray(y_pred).reshape((-1,1))\n",
    "print(y_pred.shape)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "regr.fit(X_train, y_train[:,1])\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred2= np.asarray(y_pred).reshape((-1,1))\n",
    "print(y_pred.shape)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "regr.fit(X_train, y_train[:,2])\n",
    "y_pred = regr.predict(X_test)\n",
    "y_pred3= np.asarray(y_pred).reshape((-1,1))\n",
    "print(y_pred.shape)\n",
    "\n",
    "\n",
    "y_pred = np.append(y_pred1,y_pred2, axis=1)\n",
    "y_pred = np.append(y_pred,y_pred3, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_i = scaler.inverse_transform(y_pred)\n",
    "y_test_i = scaler.inverse_transform(y_test)\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,0], y_pred_i[:,0])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,1], y_pred_i[:,1])))\n",
    "print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,2], y_pred_i[:,2])))\n",
    "rmse = math.sqrt(mean_squared_error(y_test_i, y_pred_i))\n",
    "print('Test RMSE on first future time point: %.3f' % rmse)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test_i[:,0], y_pred_i[:,0]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_i[:,1], y_pred_i[:,1]))\n",
    "print('Variance score: %.2f' % r2_score(y_test_i[:,2], y_pred_i[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVR\n",
    "# # Fit svr_rbfession model\n",
    "# svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "# svr_rbf.fit(X_train, y_train[:,0])\n",
    "\n",
    "# # Make predictions using the testing set\n",
    "# y_pred = svr_rbf.predict(X_test)\n",
    "# y_pred1= np.asarray(y_pred).reshape((-1,1))\n",
    "# # Make predictions using the testing set\n",
    "# svr_rbf.fit(X_train, y_train[:,1])\n",
    "# y_pred = svr_rbf.predict(X_test)\n",
    "# y_pred2= np.asarray(y_pred).reshape((-1,1))\n",
    "# # Make predictions using the testing set\n",
    "# svr_rbf.fit(X_train, y_train[:,2])\n",
    "# y_pred = svr_rbf.predict(X_test)\n",
    "# y_pred3= np.asarray(y_pred).reshape((-1,1))\n",
    "# y_pred = np.append(y_pred1,y_pred2, axis=1)\n",
    "# y_pred = np.append(y_pred,y_pred3, axis=1)\n",
    "# y_pred_i = scaler.inverse_transform(y_pred)\n",
    "# y_test_i = scaler.inverse_transform(y_test)\n",
    "# print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,0], y_pred_i[:,0])))\n",
    "# print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,1], y_pred_i[:,1])))\n",
    "# print(\"Mean squared error: %.2f\" % math.sqrt(mean_squared_error(y_test_i[:,2], y_pred_i[:,2])))\n",
    "# print(\"Mean squared error: %.2f\" % mean_squared_error(y_test_i, y_pred_i))\n",
    "# # Explained variance score: 1 is perfect prediction\n",
    "# print('Variance score: %.2f' % r2_score(y_test_i[:,0], y_pred_i[:,0], multioutput='variance_weighted'))\n",
    "# print('Variance score: %.2f' % r2_score(y_test_i[:,1], y_pred_i[:,1], multioutput='variance_weighted'))\n",
    "# print('Variance score: %.2f' % r2_score(y_test_i[:,2], y_pred_i[:,2], multioutput='variance_weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import median_absolute_error, r2_score\n",
    "# from sklearn.compose import TransformedTargetRegressor\n",
    "# f, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "# y_test=y_test_i[:,0]\n",
    "# y_pred=y_pred_i[:,0]\n",
    "# ax0.scatter(y_test, y_pred)\n",
    "# ax0.plot([0, 10], [0, 10], '--k')\n",
    "# ax0.set_ylabel('Target predicted')\n",
    "# ax0.set_xlabel('True Target')\n",
    "# ax0.set_title('Ridge regression \\n without target transformation')\n",
    "# ax0.text(1, 9, r'$R^2$=%.2f, MAE=%.2f' % (\n",
    "#     r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\n",
    "# ax0.set_xlim([0, 10])\n",
    "# ax0.set_ylim([0, 10])\n",
    "\n",
    "# regr_trans = TransformedTargetRegressor(\n",
    "#     regressor=RidgeCV(),\n",
    "#     transformer=QuantileTransformer(output_distribution='normal'))\n",
    "# regr_trans.fit(X_train, y_train)\n",
    "# y_pred = regr_trans.predict(X_test)\n",
    "\n",
    "# ax1.scatter(y_test, y_pred)\n",
    "# ax1.plot([0, 10], [0, 10], '--k')\n",
    "# ax1.set_ylabel('Target predicted')\n",
    "# ax1.set_xlabel('True Target')\n",
    "# ax1.set_title('Ridge regression \\n with target transformation')\n",
    "# ax1.text(1, 9, r'$R^2$=%.2f, MAE=%.2f' % (\n",
    "#     r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\n",
    "# ax1.set_xlim([0, 10])\n",
    "# ax1.set_ylim([0, 10])\n",
    "\n",
    "# f.suptitle(\"Boston housing data: distance to employment centers\", y=0.035)\n",
    "# f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
